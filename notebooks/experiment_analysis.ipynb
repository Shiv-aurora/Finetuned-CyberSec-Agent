{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CyberSecLLM: Experiment Analysis\n",
        "\n",
        "This notebook provides an interactive exploration of the CyberSecLLM project.\n",
        "\n",
        "## Contents\n",
        "1. Setup\n",
        "2. Load Results  \n",
        "3. Visualize Metrics\n",
        "4. Interactive Demo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup - Import libraries\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 11\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load metrics comparison data\n",
        "metrics_data = {\n",
        "    'Model': ['Pre-trained T5', 'Fine-tuned CyberSecLLM'],\n",
        "    'Coherence': [0.049, 0.450],\n",
        "    'Hallucination': [0.091, 0.236],\n",
        "    'Perplexity': [1.252, 1.189],\n",
        "    'Combined Score': [-0.054, 0.202]\n",
        "}\n",
        "\n",
        "df_metrics = pd.DataFrame(metrics_data)\n",
        "df_metrics.set_index('Model', inplace=True)\n",
        "print(\"Model Metrics Comparison:\")\n",
        "df_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize metrics comparison\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "x = np.arange(4)\n",
        "width = 0.35\n",
        "\n",
        "pretrained = df_metrics.loc['Pre-trained T5'].values\n",
        "finetuned = df_metrics.loc['Fine-tuned CyberSecLLM'].values\n",
        "\n",
        "bars1 = ax.bar(x - width/2, pretrained, width, label='Pre-trained T5', color='#6c757d')\n",
        "bars2 = ax.bar(x + width/2, finetuned, width, label='Fine-tuned CyberSecLLM', color='#00d4aa')\n",
        "\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('CyberSecLLM: Pre-trained vs Fine-tuned Performance', fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(['Coherence', 'Hallucination', 'Perplexity', 'Combined Score'])\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey Improvements:\")\n",
        "print(f\"  - Coherence: +818%\")\n",
        "print(f\"  - Combined Score: +474%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Demo\n",
        "\n",
        "Load the model and try asking cybersecurity questions!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to load and test the model\n",
        "# from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "# \n",
        "# MODEL_ID = \"shiv-aurora/cybersec-t5-small\"\n",
        "# \n",
        "# tokenizer = T5Tokenizer.from_pretrained(MODEL_ID)\n",
        "# model = T5ForConditionalGeneration.from_pretrained(MODEL_ID)\n",
        "# \n",
        "# def ask_question(question):\n",
        "#     prompt = f\"Answer the following cybersecurity question.\\n\\nQuestion: {question}\\nAnswer:\"\n",
        "#     inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "#     outputs = model.generate(**inputs, max_new_tokens=128, do_sample=True, top_p=0.9, temperature=0.7)\n",
        "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "# \n",
        "# # Try it!\n",
        "# question = \"What is SQL injection?\"\n",
        "# print(f\"Q: {question}\")\n",
        "# print(f\"A: {ask_question(question)}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
